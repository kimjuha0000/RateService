{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Traceback (most recent call last):\n",
       "  File \"c:\\Users\\PC\\.vscode\\extensions\\ms-python.python-2025.2.0-win32-x64\\python_files\\python_server.py\", line 133, in exec_user_input\n",
       "    retval = callable_(user_input, user_globals)\n",
       "  File \"<string>\", line 8, in <module>\n",
       "ModuleNotFoundError: No module named 'data_loader'\n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "from data_loader import download_data, load_data, clean_data\n",
    "from preprocess import preprocess_text, pad_sequences_custom\n",
    "\n",
    "# PyTorch 모델 정의: Embedding -> LSTM -> Dense\n",
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout):\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers=num_layers, \n",
    "                            batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "        out = self.fc(h_n[-1])\n",
    "        out = self.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.py\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "\n",
    "from data_loader import download_data, load_data, clean_data\n",
    "from preprocess import preprocess_text, pad_sequences_custom\n",
    "\n",
    "# PyTorch 모델 정의: Embedding -> LSTM -> Dense\n",
    "class SentimentLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout):\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers=num_layers, \n",
    "                            batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "        out = self.fc(h_n[-1])\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "def train_model(model, X_train, y_train, batch_size=64, epochs=15, patience=4, learning_rate=0.001):\n",
    "    X_tensor = torch.tensor(X_train, dtype=torch.long)\n",
    "    y_tensor = torch.tensor(y_train, dtype=torch.float).unsqueeze(1)\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    \n",
    "    # 80:20 비율로 훈련/검증 분할\n",
    "    val_size = int(0.2 * len(dataset))\n",
    "    train_size = len(dataset) - val_size\n",
    "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        for batch_x, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * batch_x.size(0)\n",
    "        train_loss /= train_size\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for batch_x, batch_y in val_loader:\n",
    "                outputs = model(batch_x)\n",
    "                loss = criterion(outputs, batch_y)\n",
    "                val_loss += loss.item() * batch_x.size(0)\n",
    "        val_loss /= val_size\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {train_loss:.4f} / Val Loss: {val_loss:.4f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            epochs_no_improve = 0\n",
    "            torch.save(model.state_dict(), \"best_model.pth\")\n",
    "            print(\"최적 모델 저장됨.\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, batch_size=64):\n",
    "    model.eval()\n",
    "    X_tensor = torch.tensor(X_test, dtype=torch.long)\n",
    "    y_tensor = torch.tensor(y_test, dtype=torch.float).unsqueeze(1)\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    loader = DataLoader(dataset, batch_size=batch_size)\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in loader:\n",
    "            outputs = model(batch_x)\n",
    "            preds = (outputs > 0.5).float()\n",
    "            correct += (preds == batch_y).sum().item()\n",
    "            total += batch_y.size(0)\n",
    "    accuracy = correct / total\n",
    "    print(f\"\\n테스트 정확도: {accuracy:.4f}\")\n",
    "\n",
    "def sentiment_predict(new_sentence, vocab, model, okt, stopwords, max_len):\n",
    "    new_sentence = re.sub(r'[^ㄱ-ㅎㅏ-ㅣ가-힣 ]', '', new_sentence)\n",
    "    tokens = okt.morphs(new_sentence, stem=True)\n",
    "    filtered = [word for word in tokens if word not in stopwords]\n",
    "    seq = [vocab[word] for word in filtered if word in vocab]\n",
    "    if len(seq) == 0:\n",
    "        print(\"입력된 문장이 너무 짧거나 분석할 수 없습니다.\")\n",
    "        return\n",
    "    seq_pad = pad_sequences_custom([seq], max_len)\n",
    "    seq_tensor = torch.tensor(seq_pad, dtype=torch.long)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(seq_tensor)\n",
    "    score = output.item()\n",
    "    if score > 0.5:\n",
    "        print(\"{:.2f}% 확률로 긍정 리뷰입니다.\\n\".format(score * 100))\n",
    "    else:\n",
    "        print(\"{:.2f}% 확률로 부정 리뷰입니다.\\n\".format((1 - score) * 100))\n",
    "\n",
    "def main():\n",
    "    # 1. 데이터 다운로드 및 로드/클리닝\n",
    "    download_data()\n",
    "    train_data, test_data = load_data()\n",
    "    train_data = clean_data(train_data)\n",
    "    test_data = clean_data(test_data)\n",
    "    \n",
    "    # 2. 전처리 (토큰화, 시퀀스 변환, 패딩 등)\n",
    "    (X_train, y_train), (X_test, y_test), vocab, okt, stopwords, max_len = preprocess_text(\n",
    "        train_data, test_data, max_len=30, threshold=3\n",
    "    )\n",
    "    \n",
    "    # 3. 모델 생성 및 훈련\n",
    "    vocab_size = len(vocab) + 1\n",
    "    embed_size = 100\n",
    "    hidden_size = 128\n",
    "    num_layers = 1\n",
    "    dropout = 0.0\n",
    "    model = SentimentLSTM(vocab_size, embed_size, hidden_size, num_layers, dropout)\n",
    "    model = train_model(model, X_train, y_train, batch_size=64, epochs=15, patience=4, learning_rate=0.001)\n",
    "    \n",
    "    # 4. 평가\n",
    "    evaluate_model(model, X_test, y_test, batch_size=64)\n",
    "    \n",
    "    # 5. 예측 테스트\n",
    "    print(\"\\n예측 결과:\")\n",
    "    sentiment_predict('이 영화 개꿀잼 ㅋㅋㅋ', vocab, model, okt, stopwords, max_len)\n",
    "    sentiment_predict('이 영화 핵노잼 ㅠㅠ', vocab, model, okt, stopwords, max_len)\n",
    "    sentiment_predict('이딴게 영화냐 ㅉㅉ', vocab, model, okt, stopwords, max_len)\n",
    "    sentiment_predict('감독 뭐하는 놈이냐?', vocab, model, okt, stopwords, max_len)\n",
    "    sentiment_predict('와 개쩐다 정말 세계관 최강자들의 영화다', vocab, model, okt, stopwords, max_len)\n",
    "    sentiment_predict('엥 이거 뭐냐', vocab, model, okt, stopwords, max_len)\n",
    "    sentiment_predict('집에가고싶다', vocab, model, okt, stopwords, max_len)\n",
    "    sentiment_predict('재미없있네요', vocab, model, okt, stopwords, max_len)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
